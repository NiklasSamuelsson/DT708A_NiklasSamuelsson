CartPole-v1
-----------
Version 0.1
- Found one bug early (see image "Bug nr 1")
- The agent still does not improve
- Goes either left or right all the time
- ANN configuration does not matter for convergence
	- Have tried Rprop, Adam, SGD...
	- ...ReLu, Sigmoid...
	- ...2 hidden layers, 3 hidden layers...
	- ...layer sizes of 5 neuron, 20 neurons
- Using MSE as loss function
- Don't use any batches in training (all samples in 1 bacth each epoch)
	- Samuel Blad succeeded with a batch size of 1024. Mine does not improve at all even before I have reached that number of samples.
- Have not adjusted the reward anything
- Using one-hot encoding for the actions
	- Input to ANN is then 6 features
- Output is one neuron containing the state-action value
- Discount is 0.9
	- 0.99 doesn't make a difference
- Epsilon is 0.1
	- Making it 100% greedy (epsilon=0) does not improve it
- Number of epochs does not change anything
	- Have tried 20, 25, 40 and 100


Version 0.2
- Found one completely game breaking bug (see image "Bug nr 2")!
	- Did not resolve the issue...
- Tried to change learning rate from 0.1 to 0.001: did not improve learning
	- Same behavior of going in just on direction
	- Also tried 0.01 but no improvements still
- Tried with only 5 epochs and 500 epochs: no improvements
- Currently using Sigmoid: impossible for the NN to reach the proper state-action values (see image "Tuning issue nr 1")
	- Like Samuel Blad said!
	- Changing to ReLu everywhere did not resolve it
	- Neither did removing the last activation function (which was ReLu)
- Tried to implement a uniform random choice as tie-breaker for equally greedy actions: no improvement
- Tried normalizing input to ANN: no improvement


Version 0.3
- Tried chaning reward to 0 for all transitions except termination (1) and also set the greedy action to minimize instead: no improvement
- Initialized the experiece buffer with uniform random action selection transition for 200 episodes: no improvement whatsoever
- Only updated Q after each episode: no difference
- Reduced ANN to only have 5 neurons per hidden layer without any improvement
- Set the target equal to the r if next state is terminal (r==1), no improvement
- Treid chaning dtype to float64 instead of torch32: no diff
- Tried 1 epoch only, no improvement
- Move the squeeze function in the ANN, no diff
- Tried recalculating target after each epoch, no change
- Tried normalizing input again, no diff
- Tried setting target to 0 for all experiences except terminal ones for the first 100 eps, no diff
	- Also tried a much bigger network with this to no avail.